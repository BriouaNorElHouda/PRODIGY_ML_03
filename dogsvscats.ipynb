{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3362,"databundleVersionId":31148,"sourceType":"competition"}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import zipfile\nimport os\nimport cv2\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import IncrementalPCA","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:35:29.872900Z","iopub.execute_input":"2024-11-23T22:35:29.873669Z","iopub.status.idle":"2024-11-23T22:35:29.877797Z","shell.execute_reply.started":"2024-11-23T22:35:29.873636Z","shell.execute_reply":"2024-11-23T22:35:29.876861Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\n# Paths to the zip files\ntrain_zip = '/kaggle/input/dogs-vs-cats/train.zip'\ntest_zip = '/kaggle/input/dogs-vs-cats/test1.zip'\n\n# Extract directories\ntrain_dir = '/kaggle/working/train'\ntest_dir = '/kaggle/working/test'\n\n# Unzipping the training dataset\nwith zipfile.ZipFile(train_zip, 'r') as zip_ref:\n    zip_ref.extractall(train_dir)\n\n# Unzipping the test dataset\nwith zipfile.ZipFile(test_zip, 'r') as zip_ref:\n    zip_ref.extractall(test_dir)\n\nprint(\"Data unzipped!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:32:25.058499Z","iopub.execute_input":"2024-11-23T22:32:25.058810Z","iopub.status.idle":"2024-11-23T22:32:34.708271Z","shell.execute_reply.started":"2024-11-23T22:32:25.058785Z","shell.execute_reply":"2024-11-23T22:32:34.707399Z"}},"outputs":[{"name":"stdout","text":"Data unzipped!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Paths to the directories\ntrain_dir = '/kaggle/working/train/train'\ntest_dir = '/kaggle/working/test'\n\n# Check the files in the train and test directories\ntrain_images = os.listdir(train_dir)\ntest1_dir = os.path.join(test_dir, 'test1')\n\n# Check contents of test1\ntest_images = os.listdir(test1_dir)\n\n# Print the number of images and a few examples to verify\nprint(f\"Number of training images: {len(train_images)}\")\nprint(f\"Number of test images: {len(test_images)}\")\n\n# Display a few examples\nprint(\"First 5 training images:\", train_images[:5])\nprint(\"First 5 test images:\", test_images[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:32:37.982089Z","iopub.execute_input":"2024-11-23T22:32:37.982768Z","iopub.status.idle":"2024-11-23T22:32:38.005723Z","shell.execute_reply.started":"2024-11-23T22:32:37.982738Z","shell.execute_reply":"2024-11-23T22:32:38.004887Z"}},"outputs":[{"name":"stdout","text":"Number of training images: 25000\nNumber of test images: 12500\nFirst 5 training images: ['cat.12225.jpg', 'cat.8047.jpg', 'cat.8037.jpg', 'dog.11216.jpg', 'dog.11261.jpg']\nFirst 5 test images: ['10377.jpg', '6213.jpg', '5471.jpg', '5134.jpg', '3819.jpg']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\n# Function to load and preprocess images\ndef load_and_preprocess_images(image_paths, target_size=(128, 128)):\n    images = []\n    for image_path in image_paths:\n        # Load image using OpenCV\n        img = cv2.imread(image_path)\n        \n        # Check if the image was loaded successfully\n        if img is None:\n            print(f\"Warning: Could not load image {image_path}\")\n            continue\n        \n        # Resize the image\n        img_resized = cv2.resize(img, target_size)\n        \n        # Normalize pixel values to [0, 1]\n        img_normalized = img_resized / 255.0\n        \n        # Flatten the image to a 1D array (for SVM)\n        img_flattened = img_normalized.flatten()\n        \n        images.append(img_flattened)\n    \n    return np.array(images)\n\n# Correct path to training images\ntrain_dir = '/kaggle/working/train/train'\ntrain_image_paths = [os.path.join(train_dir, img) for img in train_images]\nX_train = load_and_preprocess_images(train_image_paths)\n\n# Check the shape of the first preprocessed image\nprint(f\"Shape of first preprocessed image: {X_train[0].shape}\")\nprint(f\"Total number of training images: {X_train.shape[0]}\")\n\n# Correct path to test images\ntest1_dir = '/kaggle/working/test/test1'\ntest_image_paths = [os.path.join(test1_dir, img) for img in test_images]\nX_test = load_and_preprocess_images(test_image_paths)\n\nprint(f\"Total number of test images: {X_test.shape[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:32:42.672732Z","iopub.execute_input":"2024-11-23T22:32:42.673568Z","iopub.status.idle":"2024-11-23T22:33:28.844571Z","shell.execute_reply.started":"2024-11-23T22:32:42.673533Z","shell.execute_reply":"2024-11-23T22:33:28.843595Z"}},"outputs":[{"name":"stdout","text":"Shape of first preprocessed image: (49152,)\nTotal number of training images: 25000\nTotal number of test images: 12500\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\n# Step 1: Extract labels\ndef extract_labels(image_paths):\n    labels = []\n    for path in tqdm(image_paths, desc=\"Extracting labels\"):\n        filename = os.path.basename(path)\n        if filename.startswith('cat'):\n            labels.append(0)  # 0 for cats\n        elif filename.startswith('dog'):\n            labels.append(1)  # 1 for dogs\n        else:\n            print(f\"Warning: Unknown label in filename {filename}\")\n    return np.array(labels)\n\n# Extract labels for training images\ny_train = extract_labels(train_image_paths)\n\n# Step 2: Split into training and validation sets\nX_train_split, X_val, y_train_split, y_val = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=42\n)\n\nprint(f\"Training set size: {X_train_split.shape[0]}\")\nprint(f\"Validation set size: {X_val.shape[0]}\")\n\n# Optional: Reduce dimensionality with IncrementalPCA in batches\nprint(\"Applying IncrementalPCA to reduce feature dimensions...\")\nbatch_size = 16  # Use smaller batch size\nipca = IncrementalPCA(n_components=50)  # Use IncrementalPCA instead of PCA\n\n# Fit IncrementalPCA incrementally in batches\nfor start in tqdm(range(0, len(X_train_split), batch_size), desc=\"Fitting IncrementalPCA\"):\n    end = min(start + batch_size, len(X_train_split))\n    ipca.partial_fit(X_train_split[start:end])\n\n# Transform training and validation data in batches\ndef transform_in_batches(data, ipca, batch_size):\n    transformed_data = []\n    for start in tqdm(range(0, len(data), batch_size), desc=\"Transforming data\"):\n        end = min(start + batch_size, len(data))\n        transformed_data.append(ipca.transform(data[start:end]))\n    return np.vstack(transformed_data)\n\nX_train_pca = transform_in_batches(X_train_split, ipca, batch_size)\nX_val_pca = transform_in_batches(X_val, ipca, batch_size)\n\n# Step 3: Train the SVM classifier\nprint(\"Training the SVM classifier...\")\nstart_time = time.time()\n\n# SVM trains on the full PCA-transformed data (batch training not required for SVM)\nsvm_model = SVC(kernel='linear', C=1.0, random_state=42)\nsvm_model.fit(X_train_pca, y_train_split)\n\ntraining_time = time.time() - start_time\nprint(f\"Training completed in {training_time:.2f} seconds\")\n\n# Step 4: Evaluate on the validation set\nval_accuracy = svm_model.score(X_val_pca, y_val)\nprint(f\"Validation Accuracy: {val_accuracy:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:33:35.094235Z","iopub.execute_input":"2024-11-23T22:33:35.094566Z","iopub.status.idle":"2024-11-23T22:33:43.441018Z","shell.execute_reply.started":"2024-11-23T22:33:35.094536Z","shell.execute_reply":"2024-11-23T22:33:43.439882Z"}},"outputs":[{"name":"stderr","text":"Extracting labels: 100%|██████████| 25000/25000 [00:00<00:00, 1008168.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training set size: 20000\nValidation set size: 5000\nApplying PCA to reduce feature dimensions...\n","output_type":"stream"},{"name":"stderr","text":"Fitting PCA:   0%|          | 0/1250 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start \u001b[38;5;241m+\u001b[39m batch_size, \u001b[38;5;28mlen\u001b[39m(X_train_split))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m(X_train_split[start:end])  \u001b[38;5;66;03m# Initialize PCA\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     pca\u001b[38;5;241m.\u001b[39mpartial_fit(X_train_split[start:end])  \u001b[38;5;66;03m# Incremental fitting\u001b[39;00m\n","\u001b[0;31mAttributeError\u001b[0m: 'PCA' object has no attribute 'partial_fit'"],"ename":"AttributeError","evalue":"'PCA' object has no attribute 'partial_fit'","output_type":"error"}],"execution_count":6}]}