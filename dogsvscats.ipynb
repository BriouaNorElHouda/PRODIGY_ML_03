{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3362,"databundleVersionId":31148,"sourceType":"competition"}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import zipfile\nimport os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import IncrementalPCA\nimport time\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Step 2: Load and preprocess images in batches\ndef load_images_in_batches(image_paths, batch_size=16, target_size=(64, 64)):\n    for start in range(0, len(image_paths), batch_size):\n        end = min(start + batch_size, len(image_paths))\n        images = []\n        for image_path in image_paths[start:end]:\n            img = cv2.imread(image_path)\n            if img is not None:\n                img_resized = cv2.resize(img, target_size)\n                img_normalized = img_resized / 255.0\n                img_flattened = img_normalized.flatten()\n                images.append(img_flattened)\n        yield np.array(images)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Step 3: Extract labels\ndef extract_labels(image_paths):\n    labels = []\n    for path in tqdm(image_paths, desc=\"Extracting labels\"):\n        filename = os.path.basename(path)\n        if filename.startswith('cat'):\n            labels.append(0)\n        elif filename.startswith('dog'):\n            labels.append(1)\n    return np.array(labels)\n\ntrain_image_paths = [os.path.join(train_dir, 'train', img) for img in train_images]\ny_train = extract_labels(train_image_paths)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Step 4: Split into train and validation\ntrain_paths, val_paths, y_train_split, y_val = train_test_split(\n    train_image_paths, y_train, test_size=0.2, random_state=42\n)\n\n# Step 5: Incremental PCA\nprint(\"Applying IncrementalPCA to reduce feature dimensions...\")\nbatch_size = 16\nipca = IncrementalPCA(n_components=16)\n\nfor batch in tqdm(load_images_in_batches(train_paths, batch_size), desc=\"Fitting IncrementalPCA\"):\n    ipca.partial_fit(batch)\n\nX_train_pca = np.vstack([\n    ipca.transform(batch) for batch in tqdm(load_images_in_batches(train_paths, batch_size), desc=\"Transforming train data\")\n])\nX_val_pca = np.vstack([\n    ipca.transform(batch) for batch in tqdm(load_images_in_batches(val_paths, batch_size), desc=\"Transforming validation data\")\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Step 6: Train the SVM\nprint(\"Training the SVM classifier...\")\nstart_time = time.time()\nsvm_model = SVC(kernel='rbf', C=100.0, random_state=42)\nsvm_model.fit(X_train_pca, y_train_split)\ntraining_time = time.time() - start_time\nprint(f\"Training completed in {training_time:.2f} seconds\")\n\n# Step 7: Evaluate\nval_accuracy = svm_model.score(X_val_pca, y_val)\nprint(f\"Validation Accuracy: {val_accuracy:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:17:40.607823Z","iopub.execute_input":"2024-11-24T19:17:40.608296Z"}},"outputs":[{"name":"stdout","text":"Training the SVM classifier...\n","output_type":"stream"}],"execution_count":null}]}